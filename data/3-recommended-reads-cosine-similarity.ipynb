{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737d35c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T09:35:58.630138Z",
     "iopub.status.busy": "2023-05-20T09:35:58.629265Z",
     "iopub.status.idle": "2023-05-20T09:36:15.726944Z",
     "shell.execute_reply": "2023-05-20T09:36:15.725250Z"
    },
    "papermill": {
     "duration": 17.104786,
     "end_time": "2023-05-20T09:36:15.729698",
     "exception": false,
     "start_time": "2023-05-20T09:35:58.624912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/2-recommended-reads-conversion-of-data-to-num/__results__.html\n",
      "/kaggle/input/2-recommended-reads-conversion-of-data-to-num/vectorizedData.csv\n",
      "/kaggle/input/2-recommended-reads-conversion-of-data-to-num/__notebook__.ipynb\n",
      "/kaggle/input/2-recommended-reads-conversion-of-data-to-num/__output__.json\n",
      "/kaggle/input/2-recommended-reads-conversion-of-data-to-num/custom.css\n",
      "/kaggle/input/2-recommended-reads-conversion-of-data-to-num/__results___files/__results___9_0.png\n",
      "/kaggle/input/googlenewsvectors/GoogleNews-vectors-negative300.bin\n",
      "Collecting sentence-transformers\r\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.28.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0+cpu)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1+cpu)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.9.3)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.98)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.13.4)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.3.23)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\r\n",
      "Building wheels for collected packages: sentence-transformers\r\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=4deee0fe82777c2c7cc2951254ae49bd054485c90bed17ba2814f262e462c0fb\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\r\n",
      "Successfully built sentence-transformers\r\n",
      "Installing collected packages: sentence-transformers\r\n",
      "Successfully installed sentence-transformers-2.2.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import pandas as pd import string import gensim import nltk from sklearn.metrics.pairwise import cosine_similarity nltk.download('stopwords') nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import os \n",
    "for dirname, _, filenames in os.walk('/kaggle/input'): \n",
    "    for filename in filenames: \n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff7f371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T09:36:15.738655Z",
     "iopub.status.busy": "2023-05-20T09:36:15.738232Z",
     "iopub.status.idle": "2023-05-20T09:36:16.030939Z",
     "shell.execute_reply": "2023-05-20T09:36:16.029804Z"
    },
    "papermill": {
     "duration": 0.300215,
     "end_time": "2023-05-20T09:36:16.033644",
     "exception": false,
     "start_time": "2023-05-20T09:36:15.733429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "def similar_books(user_input):\n",
    "    model_path = \"/kaggle/input/googlenewsvectors/GoogleNews-vectors-negative300.bin\"\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "    books = pd.read_csv(\"/kaggle/input/2-recommended-reads-conversion-of-data-to-num/vectorizedData.csv\")\n",
    "\n",
    "    # Clean and tokenize user input\n",
    "    stop_words = stopwords.words('english')\n",
    "    user_tokens = [token.lower() for token in word_tokenize(user_input) if token.isalpha() and token.lower() not in stop_words]\n",
    "    cleaned_input = ' '.join(user_tokens)\n",
    "\n",
    "    # Vectorize cleaned user input\n",
    "    input_vector = np.zeros((model.vector_size,))\n",
    "    count = 0\n",
    "    for token in cleaned_input.split():\n",
    "        if token in model:\n",
    "            input_vector += model[token]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        input_vector /= count\n",
    "\n",
    "    # Clean and tokenize book descriptions and vectorize\n",
    "    book_vectors = []\n",
    "    book_titles = []\n",
    "    book_authors = []\n",
    "    for title, author, desc in zip(books['booktitle'], books['authorname'], books['bookdescription']):\n",
    "        desc_tokens = [token.lower() for token in word_tokenize(desc) if token.isalpha() and token.lower() not in stop_words]\n",
    "        cleaned_desc = ' '.join(desc_tokens)\n",
    "        if cleaned_desc:\n",
    "            desc_vector = np.zeros((model.vector_size,))\n",
    "            count = 0\n",
    "            for token in cleaned_desc.split():\n",
    "                if token in model:\n",
    "                    desc_vector += model[token]\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                desc_vector /= count\n",
    "                book_vectors.append(desc_vector)\n",
    "                book_titles.append(title)\n",
    "                book_authors.append(author)\n",
    "\n",
    "    # Compute cosine similarities between input vector and book vectors\n",
    "    similarities = cosine_similarity([input_vector], book_vectors)\n",
    "\n",
    "    # Combine book data and similarity scores into DataFrame\n",
    "    book_data = pd.DataFrame({'booktitle': book_titles, 'authorname': book_authors, 'bookdescription': books['bookdescription'][:len(book_vectors)], 'similarity': similarities[0]})\n",
    "\n",
    "    # Remove duplicate books based on title and author\n",
    "    book_data.drop_duplicates(subset=['booktitle', 'authorname'], inplace=True)\n",
    "\n",
    "    # Sort by similarity and return top 5 books\n",
    "    similar_books = book_data.sort_values(by='similarity', ascending=False).head(5)\n",
    "\n",
    "    # Generate recommendation string\n",
    "    recommendation = \"Recommended books based on your input:\\n\"\n",
    "    for index, row in similar_books.iterrows():\n",
    "        recommendation += f\"{row['booktitle']} by {row['authorname']} - Similarity: {round(row['similarity'], 3)}\\n\"\n",
    "\n",
    "    print(recommendation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd91464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T09:36:16.043758Z",
     "iopub.status.busy": "2023-05-20T09:36:16.043322Z",
     "iopub.status.idle": "2023-05-20T09:37:41.709766Z",
     "shell.execute_reply": "2023-05-20T09:37:41.708780Z"
    },
    "papermill": {
     "duration": 85.676669,
     "end_time": "2023-05-20T09:37:41.714290",
     "exception": false,
     "start_time": "2023-05-20T09:36:16.037621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended books based on your input:\n",
      "Field Trip to the Moon by John L. Hare - Similarity: 0.644\n",
      "Space Chantey by R.A. Lafferty - Similarity: 0.643\n",
      "Odyssey One by Evan Currie - Similarity: 0.621\n",
      "A Love Through Time by Terri Brisbin - Similarity: 0.62\n",
      "The Heart of Matter by Evan Currie - Similarity: 0.619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = \"a space journey with friends\"\n",
    "similar_books(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368901c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T09:37:41.723193Z",
     "iopub.status.busy": "2023-05-20T09:37:41.722810Z",
     "iopub.status.idle": "2023-05-20T09:37:41.727876Z",
     "shell.execute_reply": "2023-05-20T09:37:41.726465Z"
    },
    "papermill": {
     "duration": 0.012825,
     "end_time": "2023-05-20T09:37:41.730463",
     "exception": false,
     "start_time": "2023-05-20T09:37:41.717638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop same name books from data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 115.824892,
   "end_time": "2023-05-20T09:37:43.059206",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-20T09:35:47.234314",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
